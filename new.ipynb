{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T19:00:06.966072Z",
     "start_time": "2024-03-04T19:00:06.959346Z"
    }
   },
   "id": "8c5a49fa0052482f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to extract course details\n",
    "def extract_course_details(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract department\n",
    "    department = soup.find('span', string=\"Кафедра:\").find_next('a').text.strip()\n",
    "\n",
    "    # Extract lecturer\n",
    "    lecturer_info = soup.find('td', class_='lecturer')\n",
    "    if lecturer_info:\n",
    "        lecturer_link_element = lecturer_info.find('a')\n",
    "        if lecturer_link_element:  # Check if the <a> tag exists within lecturer_info\n",
    "            lecturer = lecturer_link_element.text.strip()\n",
    "            lecturer_link = lecturer_link_element['href']\n",
    "        else:\n",
    "            lecturer = \"Lecturer information not available\"\n",
    "            lecturer_link = \"No link available\"\n",
    "    else:\n",
    "        lecturer = \"Lecturer information not available\"\n",
    "        lecturer_link = \"No link available\"\n",
    "\n",
    "\n",
    "    # Extract recommended literature\n",
    "    literature_section = soup.find('section', class_='materials')\n",
    "    if literature_section:  # Check if the literature_section is not None\n",
    "        literature = [li.text.strip() for li in literature_section.find_all('li')]\n",
    "    else:\n",
    "        literature = [\"No literature information available\"]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'department': department,\n",
    "        'lecturer': {\n",
    "            'name': lecturer,\n",
    "            'link': lecturer_link,\n",
    "        },\n",
    "        'recommended_literature': literature,\n",
    "    }\n",
    "\n",
    "# Main URL (replace with the actual curriculum page containing course links)\n",
    "main_url = 'https://kultart.lnu.edu.ua/academics/bachelor/curriculum-theatre-and-cinema'\n",
    "main_response = requests.get(main_url)\n",
    "main_soup = BeautifulSoup(main_response.content, 'html.parser')\n",
    "\n",
    "# Extract all course links\n",
    "course_links = [a['href'] for a in main_soup.select('td.title a') if 'course' in a['href']]\n",
    "\n",
    "# Loop through each link and extract details\n",
    "course_details = []\n",
    "for link in course_links:\n",
    "    details = extract_course_details(link)\n",
    "    details['course_link'] = link  # Add the course link to details\n",
    "    course_details.append(details)\n",
    "\n",
    "# Save to JSON\n",
    "with open('course_details.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(course_details, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T18:59:54.795056Z",
     "start_time": "2024-03-04T18:59:36.134817Z"
    }
   },
   "id": "dceec0e4bc1577e9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "base_url = 'https://kultart.lnu.edu.ua'\n",
    "academics_url = f'{base_url}/academics/bachelor'\n",
    "response = requests.get(academics_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "specialties = soup.find_all('section', class_='specialization')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T18:59:56.017805Z",
     "start_time": "2024-03-04T18:59:55.885230Z"
    }
   },
   "id": "3d620aabb2160a39",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fetch_specialty_details(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract department\n",
    "    department = soup.find('span', string=\"Кафедра:\").find_next('a').text.strip()\n",
    "\n",
    "    # Extract lecturer\n",
    "    lecturer_info = soup.find('td', class_='lecturer')\n",
    "    if lecturer_info:\n",
    "        lecturer_link_element = lecturer_info.find('a')\n",
    "        if lecturer_link_element:  # Check if the <a> tag exists within lecturer_info\n",
    "            lecturer = lecturer_link_element.text.strip()\n",
    "            lecturer_link = lecturer_link_element['href']\n",
    "        else:\n",
    "            lecturer = \"Lecturer information not available\"\n",
    "            lecturer_link = \"No link available\"\n",
    "    else:\n",
    "        lecturer = \"Lecturer information not available\"\n",
    "        lecturer_link = \"No link available\"\n",
    "\n",
    "\n",
    "    # Extract recommended literature\n",
    "    literature_section = soup.find('section', class_='materials')\n",
    "    if literature_section:  # Check if the literature_section is not None\n",
    "        literature = [li.text.strip() for li in literature_section.find_all('li')]\n",
    "    else:\n",
    "        literature = [\"No literature information available\"]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'department': department,\n",
    "        'lecturer': {\n",
    "            'name': lecturer,\n",
    "            'link': lecturer_link,\n",
    "        },\n",
    "        'recommended_literature': literature,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T18:59:57.410569Z",
     "start_time": "2024-03-04T18:59:57.404586Z"
    }
   },
   "id": "8d3a9b20fdc81c57",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_next'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m link \u001B[38;5;241m=\u001B[39m base_url \u001B[38;5;241m+\u001B[39m title_tag[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhref\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Fetch and process each specialty's page details\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m specialty_details \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_specialty_details\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlink\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Construct a filename based on the specialty's title\u001B[39;00m\n\u001B[1;32m     10\u001B[0m filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtitle\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;250m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mlower()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m, in \u001B[0;36mfetch_specialty_details\u001B[0;34m(url)\u001B[0m\n\u001B[1;32m      3\u001B[0m soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(response\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Extract department\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m department \u001B[38;5;241m=\u001B[39m \u001B[43msoup\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mspan\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mКафедра:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfind_next\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Extract lecturer\u001B[39;00m\n\u001B[1;32m      9\u001B[0m lecturer_info \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtd\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlecturer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'find_next'"
     ]
    }
   ],
   "source": [
    "for spec in specialties:\n",
    "    title_tag = spec.find('h3', class_='title').find('a')\n",
    "    title = title_tag.text\n",
    "    link = base_url + title_tag['href']\n",
    "    \n",
    "    # Fetch and process each specialty's page details\n",
    "    specialty_details = fetch_specialty_details(link)\n",
    "    \n",
    "    # Construct a filename based on the specialty's title\n",
    "    filename = f\"{title.replace(' ', '_').replace('.', '').lower()}.json\"\n",
    "    \n",
    "    # Save the details to a JSON file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(specialty_details, f, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T18:59:58.922190Z",
     "start_time": "2024-03-04T18:59:58.695309Z"
    }
   },
   "id": "a7e52dad6cafa72e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "28d601dffd25ae65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
